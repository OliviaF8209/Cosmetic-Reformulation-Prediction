## Notebook 2: Feature Engineering & Modeling

### 1. Feature Engineering
Here, I created new numerical features from the raw data to capture meaningful insights for the model.

Python

# Import necessary libraries
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.metrics import precision_recall_curve, auc, roc_curve, roc_auc_score, classification_report
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from imblearn.over_sampling import SMOTE

# Reload the cleaned dataset (replace with your file path)
# Note: Upload your 'updated_cosmo_chemicals.csv' to Colab's file system.
df = pd.read_csv("updated_cosmo_chemicals.csv")

# Create time-based features
df['ChemicalAge'] = (df['MostRecentDateReported'] - df['InitialDateReported']).dt.days
df['ProductLifespan'] = (df['DiscontinuedDate'] - df['InitialDateReported']).dt.days

# Create a hazard score based on domain knowledge (example based on your reports)
# Note: This is a simplified example based on your reports and presentations
def create_hazard_score(row):
    if 'carcinogen' in str(row['HazardType']).lower():
        return 10
    elif 'reproductive' in str(row['HazardType']).lower():
        return 8
    else:
        return 5

df['HazardScore'] = df.apply(create_hazard_score, axis=1)

# Display new features
print("Features after engineering:")
print(df[['ChemicalAge', 'ProductLifespan', 'HazardScore']].head())
Markdown

### 2. Addressing Class Imbalance & Model Selection
The dataset's severe class imbalance (`2.6%` reformulated) required careful handling to prevent biased models.
Python

# Prepare data for modeling
X = df.drop('Reformulated', axis=1)
y = df['Reformulated']

# Select key numerical features
X_numeric = X[['ChemicalAge', 'ProductLifespan', 'HazardScore']].dropna()
y_numeric = y[X_numeric.index]

# Split data
X_train, X_test, y_train, y_test = train_test_split(X_numeric, y_numeric, test_size=0.2, random_state=42, stratify=y_numeric)

# Apply SMOTE to training data to address imbalance
smote = SMOTE(random_state=42)
X_train_smote, y_train_smote = smote.fit_resample(X_train, y_train)

# --- Model Training & Evaluation ---
# Baseline Model: Logistic Regression
log_reg = LogisticRegression(solver='liblinear', random_state=42)
log_reg.fit(X_train_smote, y_train_smote)
y_pred_log_reg = log_reg.predict(X_test)
y_prob_log_reg = log_reg.predict_proba(X_test)[:, 1]

# Tree-based Models (Example from your notebooks)
# Note: Initial tests showed these models overfitting, achieving perfect scores.
# This code block represents the model setup, not the misleading perfect results.
rf_model = RandomForestClassifier(random_state=42)
rf_model.fit(X_train_smote, y_train_smote)
